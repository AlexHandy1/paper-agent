{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary notes\n",
    "- Strong logistic regression benchmarks for \"Relevant\" predictions (>80% accuracy). Performs better than Xgboost and BERT [in google colab] (without extensive hyperparameter tuning).\n",
    "- Could improve further by tidying up input labeling (e.g. remove all vision / general neural network stuff)\n",
    "- Should also implement full cross validation to ensure consistent performance across sample (random tests suggests does meaningfully differ)\n",
    "- Adding query and source to title feature both as embeddings and dummy vars only very marginally improves performance (so leave out in first prod version)\n",
    "- Predicting \"Read\" harder as <5% of cases and probably inconsistent rationale (basic log reg model just predicts all not read), so more of an outlier detection piece (may need different approach)\n",
    "- Difficult deployment / file size constraints on AWS Lambda where currently have to upload all python depedencies (huge files for sentence transforers / embeddings). Might have to use a separate endpoint call. In meantime, trying sklearn word count approach)\n",
    "\n",
    "#### Next steps\n",
    "- Add to AWS - need to adopt new approach to get around filesize constraints\n",
    "- Extend multi-feature approach to BERT + explore more tuning to improve performance\n",
    "- Add abstract\n",
    "- Tidy the data input so have more consistent labeling\n",
    "- Research data outlier techniques to improve \"Read\" prediction or class weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold, cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions (could move)\n",
    "\n",
    "def convert_outcome_to_bin(x):\n",
    "    if x == \"Y\":\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def evaluate(target, prediction, prediction_prob):\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "\n",
    "    # Calculate the AUC score\n",
    "    auc_score = roc_auc_score(target, prediction_prob)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(target, prediction)\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    true_negative = conf_matrix[0, 0]\n",
    "    false_positive = conf_matrix[0, 1]\n",
    "    false_negative = conf_matrix[1, 0]\n",
    "    true_positive = conf_matrix[1, 1]\n",
    "\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"AUC: {auc_score:.2f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "#NEED TO ABSTRACT TO TARGET OUTCOME\n",
    "def flag_pred_error(x):\n",
    "    target = x[\"relevant_true\"]\n",
    "    pred = x[\"relevant_pred\"]\n",
    "    \n",
    "    if pred == target:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Published</th>\n",
       "      <th>Link</th>\n",
       "      <th>Source</th>\n",
       "      <th>Query</th>\n",
       "      <th>SearchDate</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic Check</th>\n",
       "      <th>Topic Check Query</th>\n",
       "      <th>Relevant?</th>\n",
       "      <th>Read?</th>\n",
       "      <th>In Read - current</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detecting contradictions from IoT protocol spe...</td>\n",
       "      <td>Due to the boom of Internet of Things (IoT) in...</td>\n",
       "      <td>2023-Apr-29</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37164876</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>12_05_2023_19_50_17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes: \"Our approach can automatically parse the...</td>\n",
       "      <td>Yes: \"Our approach can automatically parse the...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Learning to Refine the Identification of ...</td>\n",
       "      <td>Background: Identifying practice-ready evidenc...</td>\n",
       "      <td>2023-May-08</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37164244</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>Large language model evaluation</td>\n",
       "      <td>12_05_2023_19_50_17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes: \"Newer approaches to support evidence dis...</td>\n",
       "      <td>Large language model</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evaluating Large Language Models on Medical Ev...</td>\n",
       "      <td>Recent advances in large language models (LLMs...</td>\n",
       "      <td>2023-Apr-24</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37162998</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>Large language model evaluation</td>\n",
       "      <td>12_05_2023_19_50_17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes: \"Recent advances in large language models...</td>\n",
       "      <td>Large language model</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Designing highly potent compounds using a chem...</td>\n",
       "      <td>Compound potency prediction is a major task in...</td>\n",
       "      <td>2023-May-07</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37150793</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>Large language model evaluation</td>\n",
       "      <td>12_05_2023_19_50_17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes: \"Therefore, a chemical language model was...</td>\n",
       "      <td>Large language model</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parsing within &amp; between-person dynamics of th...</td>\n",
       "      <td>Homework is a key theoretical component of cog...</td>\n",
       "      <td>2023-Apr-26</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37148652</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>Large language model evaluation</td>\n",
       "      <td>12_05_2023_19_50_17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No.</td>\n",
       "      <td>Large language model</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>Age at Menopause, Leukocyte Telomere Length, a...</td>\n",
       "      <td>Premature menopause is a risk factor for accel...</td>\n",
       "      <td>2023-Jul-25</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37489536</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>menopause prediction</td>\n",
       "      <td>26_07_2023_09_00_32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>Development and verification of the Menopause ...</td>\n",
       "      <td>A detailed, well-validated scale for measuring...</td>\n",
       "      <td>2023-Jul-25</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37490658</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>menopause symptoms</td>\n",
       "      <td>26_07_2023_09_00_32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>Translation, cross-cultural adaptation, and va...</td>\n",
       "      <td>This study describes translation, cross-cultur...</td>\n",
       "      <td>2023-Jul-25</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37490656</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>menopause symptoms</td>\n",
       "      <td>26_07_2023_09_00_32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>Sexual health in long-term breast cancer survi...</td>\n",
       "      <td>Sexual health is an important aspect of qualit...</td>\n",
       "      <td>2023-Jul-25</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37490170</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>menopause symptoms</td>\n",
       "      <td>26_07_2023_09_00_32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>The Effect of Salvia Officinalis on Hot Flashe...</td>\n",
       "      <td>The experience of hot flashes during menopause...</td>\n",
       "      <td>2023-Jul-</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/37489230</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>menopause symptoms</td>\n",
       "      <td>26_07_2023_09_00_32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1329 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     Detecting contradictions from IoT protocol spe...   \n",
       "1     Deep Learning to Refine the Identification of ...   \n",
       "2     Evaluating Large Language Models on Medical Ev...   \n",
       "3     Designing highly potent compounds using a chem...   \n",
       "4     Parsing within & between-person dynamics of th...   \n",
       "...                                                 ...   \n",
       "1324  Age at Menopause, Leukocyte Telomere Length, a...   \n",
       "1325  Development and verification of the Menopause ...   \n",
       "1326  Translation, cross-cultural adaptation, and va...   \n",
       "1327  Sexual health in long-term breast cancer survi...   \n",
       "1328  The Effect of Salvia Officinalis on Hot Flashe...   \n",
       "\n",
       "                                               Abstract    Published  \\\n",
       "0     Due to the boom of Internet of Things (IoT) in...  2023-Apr-29   \n",
       "1     Background: Identifying practice-ready evidenc...  2023-May-08   \n",
       "2     Recent advances in large language models (LLMs...  2023-Apr-24   \n",
       "3     Compound potency prediction is a major task in...  2023-May-07   \n",
       "4     Homework is a key theoretical component of cog...  2023-Apr-26   \n",
       "...                                                 ...          ...   \n",
       "1324  Premature menopause is a risk factor for accel...  2023-Jul-25   \n",
       "1325  A detailed, well-validated scale for measuring...  2023-Jul-25   \n",
       "1326  This study describes translation, cross-cultur...  2023-Jul-25   \n",
       "1327  Sexual health is an important aspect of qualit...  2023-Jul-25   \n",
       "1328  The experience of hot flashes during menopause...    2023-Jul-   \n",
       "\n",
       "                                          Link  Source  \\\n",
       "0     https://pubmed.ncbi.nlm.nih.gov/37164876  PubMed   \n",
       "1     https://pubmed.ncbi.nlm.nih.gov/37164244  PubMed   \n",
       "2     https://pubmed.ncbi.nlm.nih.gov/37162998  PubMed   \n",
       "3     https://pubmed.ncbi.nlm.nih.gov/37150793  PubMed   \n",
       "4     https://pubmed.ncbi.nlm.nih.gov/37148652  PubMed   \n",
       "...                                        ...     ...   \n",
       "1324  https://pubmed.ncbi.nlm.nih.gov/37489536  PubMed   \n",
       "1325  https://pubmed.ncbi.nlm.nih.gov/37490658  PubMed   \n",
       "1326  https://pubmed.ncbi.nlm.nih.gov/37490656  PubMed   \n",
       "1327  https://pubmed.ncbi.nlm.nih.gov/37490170  PubMed   \n",
       "1328  https://pubmed.ncbi.nlm.nih.gov/37489230  PubMed   \n",
       "\n",
       "                                Query           SearchDate  Category  \\\n",
       "0                              PubMed  12_05_2023_19_50_17       NaN   \n",
       "1     Large language model evaluation  12_05_2023_19_50_17       NaN   \n",
       "2     Large language model evaluation  12_05_2023_19_50_17       NaN   \n",
       "3     Large language model evaluation  12_05_2023_19_50_17       NaN   \n",
       "4     Large language model evaluation  12_05_2023_19_50_17       NaN   \n",
       "...                               ...                  ...       ...   \n",
       "1324             menopause prediction  26_07_2023_09_00_32       NaN   \n",
       "1325               menopause symptoms  26_07_2023_09_00_32       NaN   \n",
       "1326               menopause symptoms  26_07_2023_09_00_32       NaN   \n",
       "1327               menopause symptoms  26_07_2023_09_00_32       NaN   \n",
       "1328               menopause symptoms  26_07_2023_09_00_32       NaN   \n",
       "\n",
       "                                            Topic Check  \\\n",
       "0     Yes: \"Our approach can automatically parse the...   \n",
       "1     Yes: \"Newer approaches to support evidence dis...   \n",
       "2     Yes: \"Recent advances in large language models...   \n",
       "3     Yes: \"Therefore, a chemical language model was...   \n",
       "4                                                   No.   \n",
       "...                                                 ...   \n",
       "1324                                                NaN   \n",
       "1325                                                NaN   \n",
       "1326                                                NaN   \n",
       "1327                                                NaN   \n",
       "1328                                                NaN   \n",
       "\n",
       "                                      Topic Check Query Relevant? Read?  \\\n",
       "0     Yes: \"Our approach can automatically parse the...         N     N   \n",
       "1                                  Large language model         Y     Y   \n",
       "2                                  Large language model         Y     N   \n",
       "3                                  Large language model         Y     N   \n",
       "4                                  Large language model         N     N   \n",
       "...                                                 ...       ...   ...   \n",
       "1324                                                NaN         Y     Y   \n",
       "1325                                                NaN         Y     N   \n",
       "1326                                                NaN         Y     N   \n",
       "1327                                                NaN         Y     N   \n",
       "1328                                                NaN         Y     N   \n",
       "\n",
       "     In Read - current  \n",
       "0                    N  \n",
       "1                    Y  \n",
       "2                    N  \n",
       "3                    N  \n",
       "4                    N  \n",
       "...                ...  \n",
       "1324                 Y  \n",
       "1325                 N  \n",
       "1326                 N  \n",
       "1327                 N  \n",
       "1328                 N  \n",
       "\n",
       "[1329 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "agent_list = pd.read_csv(\"paper_agent_list_260723.csv\")\n",
    "agent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy abstract so could use as an input feature\n",
    "agent_list[\"Abstract_clean\"] = agent_list[\"Abstract\"].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    684\n",
      "1    645\n",
      "Name: relevant_bin, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1281\n",
       "1      48\n",
       "Name: read_bin, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert target outcomes to binary features\n",
    "\n",
    "agent_list[\"relevant_bin\"] = agent_list[\"Relevant?\"].apply(convert_outcome_to_bin)\n",
    "print(agent_list[\"relevant_bin\"].value_counts())\n",
    "\n",
    "agent_list[\"read_bin\"] = agent_list[\"Read?\"].apply(convert_outcome_to_bin)\n",
    "agent_list[\"read_bin\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count matrix as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data\n",
    "\n",
    "input_label = \"Title\"\n",
    "# input_label = \"Abstract_clean\" \n",
    "    #marginally more accurate (much better sensitivity e.g. misses less relevant articles), in optimum model should aim to include title, abstract and original query. \n",
    "    #Interestingly exactly the same number of errors (43 - suggests labeling inconsistency?)\n",
    "outcome_label = \"relevant_bin\"\n",
    "# outcome_label = \"read_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_label].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CountVectorizer to convert text data into numerical features\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model\n",
    "lr_model_cm = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_cm.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_cm.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_cm.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_cm.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_cm.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "AUC: 0.89\n",
      "Sensitivity: 0.74\n",
      "Specificity: 0.93\n",
      "Confusion Matrix:\n",
      "[[130  10]\n",
      " [ 33  93]]\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surface incorrect examples\n",
    "#NOTE: some of this suggests labeling error\n",
    "\n",
    "test_examples_dict = {\"titles\": X_test, \"relevant_true\": y_test, \"relevant_pred\":y_pred}\n",
    "test_examples_df = pd.DataFrame(test_examples_dict)\n",
    "\n",
    "test_examples_df[\"pred_error\"] = test_examples_df.apply(flag_pred_error, axis=1)\n",
    "test_examples_df_errors = test_examples_df[test_examples_df[\"pred_error\"] == 1]\n",
    "test_examples_df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: actually don't disagree with many of these, further supports that for best performance will likely need to refine input labels\n",
    "test_examples_df_errors[test_examples_df_errors[\"relevant_true\"] == 0][\"titles\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0312593 , 0.02247977, 0.02771401, 0.02752686, 0.02346992,\n",
       "        0.02345324, 0.027529  ]),\n",
       " 'score_time': array([0.00270486, 0.00248218, 0.00237393, 0.0023942 , 0.00240898,\n",
       "        0.00223088, 0.00239515]),\n",
       " 'test_accuracy': array([0.75263158, 0.77894737, 0.83684211, 0.72631579, 0.87368421,\n",
       "        0.83684211, 0.83068783]),\n",
       " 'train_accuracy': array([0.98858648, 0.99209833, 0.98595259, 0.98946444, 0.98946444,\n",
       "        0.98946444, 0.98859649]),\n",
       " 'test_roc_auc': array([0.80146406, 0.83107808, 0.92668589, 0.80479148, 0.93777728,\n",
       "        0.90422348, 0.8822277 ]),\n",
       " 'train_roc_auc': array([0.99963895, 0.99952786, 0.99931185, 0.99960964, 0.99969141,\n",
       "        0.99961577, 0.99952867]),\n",
       " 'test_precision': array([0.80821918, 0.79069767, 0.87654321, 0.8030303 , 0.88636364,\n",
       "        0.8974359 , 0.875     ]),\n",
       " 'train_precision': array([0.99270073, 0.99275362, 0.99085923, 0.99271403, 0.99451554,\n",
       "        0.99090909, 0.99270073]),\n",
       " 'test_recall': array([0.64130435, 0.73913043, 0.77173913, 0.57608696, 0.84782609,\n",
       "        0.75268817, 0.76086957]),\n",
       " 'train_recall': array([0.98372514, 0.99095841, 0.9801085 , 0.98553345, 0.98372514,\n",
       "        0.98731884, 0.98372514])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model (cross validation test - whole dataset)\n",
    "\n",
    "# Encode data\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_cv = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "#good article on why don't select final instance of chosen model using CV, positioned as a evaluation tool only - https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n",
    "    #also suggests that for final model could train it on the whole dataset (thereby maximising the data use)\n",
    "outputs = cross_validate(lr_model_cv, X_vectorized, y, cv=7, scoring=scoring, return_train_score=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9864559819413092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model on all data\n",
    "\n",
    "lr_model_wc = LogisticRegression().fit(X_vectorized, y)\n",
    "\n",
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "lr_model_wc_filename = 'pa_lr_model_wc_all_data_' + today_date + \".pkl\"\n",
    "pickle.dump(lr_model_wc, open(lr_model_wc_filename, 'wb'))\n",
    "\n",
    "#test load (comment out)\n",
    "lr_model_wc_loaded = pickle.load(open(lr_model_wc_filename, 'rb'))\n",
    "loaded_result = lr_model_wc_loaded.score(X_vectorized, y)\n",
    "print(loaded_result)\n",
    "\n",
    "lr_model_wc_loaded.predict(X_vectorized[2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings as feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data\n",
    "\n",
    "input_label = \"Title\"\n",
    "# input_label = \"Abstract_clean\" \n",
    "    #Takes significantly longer to embed - something to factor into application (would need to add an order of magnitude more accuracy to be worth computing on the fly)\n",
    "    #Actually performs worse on this subset\n",
    "outcome_label = \"relevant_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_label].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#generate sentence embeddings\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode train \n",
    "X_train_vectorized = encoder.encode(X_train)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_vectorized = encoder.encode(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (basic train test split to review cases / errors)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_we = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_we.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_we.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_we.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_we.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_we.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "lr_model_we_filename = 'pa_lr_model_we_tt_data_' + today_date + \".pkl\"\n",
    "pickle.dump(lr_model_we, open(lr_model_we_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load (comment out)\n",
    "lr_model_we_loaded = pickle.load(open(lr_model_we_filename, 'rb'))\n",
    "loaded_result = lr_model_we_loaded.score(X_test_vectorized, y_test)\n",
    "print(loaded_result)\n",
    "\n",
    "lr_model_we_loaded.predict(X_test_vectorized[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect errors\n",
    "\n",
    "test_examples_dict = {\"titles\": X_test, \"relevant_true\": y_test, \"relevant_pred\":y_pred}\n",
    "test_examples_df = pd.DataFrame(test_examples_dict)\n",
    "\n",
    "test_examples_df[\"pred_error\"] = test_examples_df.apply(flag_pred_error, axis=1)\n",
    "test_examples_df_errors = test_examples_df[test_examples_df[\"pred_error\"] == 1]\n",
    "test_examples_df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (cross validation test - whole dataset)\n",
    "\n",
    "# Encode data\n",
    "X_vectorized = encoder.encode(X)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_cv = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "#good article on why don't select final instance of chosen model using CV, positioned as a evaluation tool only - https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n",
    "    #also suggests that for final model could train it on the whole dataset (thereby maximising the data use)\n",
    "outputs = cross_validate(lr_model_cv, X_vectorized, y, cv=7, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model fit on entire dataset (refer to CV for estimated performance)\n",
    "\n",
    "lr_model_we_all = LogisticRegression()\n",
    "lr_model_we_all.fit(X_vectorized, y)\n",
    "\n",
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "lr_model_we_all_filename = 'pa_lr_model_we_all_data_' + today_date + \".pkl\"\n",
    "pickle.dump(lr_model_we_all, open(lr_model_we_all_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load (comment out)\n",
    "lr_model_we_all_loaded = pickle.load(open(lr_model_we_all_filename, 'rb'))\n",
    "loaded_result = lr_model_we_all_loaded.score(X_vectorized, y)\n",
    "print(loaded_result)\n",
    "\n",
    "lr_model_we_all_loaded.predict(X_vectorized[2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_we = xgb.XGBRegressor(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "xgb_model_we.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train_prob = xgb_model_we.predict(X_train_vectorized)\n",
    "y_pred_train = [ 1 if p >= 0.5 else 0 for p in y_pred_train_prob ]\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = xgb_model_we.predict(X_test_vectorized)\n",
    "y_pred = [ 1 if p >= 0.5 else 0 for p in y_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "xgb_model_we_filename = 'pa_xgb_model_we_' + today_date + \".pkl\"\n",
    "pickle.dump(xgb_model_we, open(xgb_model_we_filename, 'wb'))\n",
    "\n",
    "#test load (comment out)\n",
    "\n",
    "# xgb_model_we_loaded = pickle.load(open(xgb_model_we_filename, 'rb'))\n",
    "# loaded_result = xgb_model_we_loaded.score(X_test_vectorized, y_test)\n",
    "# print(loaded_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect errors\n",
    "\n",
    "test_examples_dict = {\"titles\": X_test, \"relevant_true\": y_test, \"relevant_pred\":y_pred}\n",
    "test_examples_df = pd.DataFrame(test_examples_dict)\n",
    "\n",
    "test_examples_df[\"pred_error\"] = test_examples_df.apply(flag_pred_error, axis=1)\n",
    "test_examples_df_errors = test_examples_df[test_examples_df[\"pred_error\"] == 1]\n",
    "test_examples_df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries and sources - as word embeddings\n",
    "\n",
    "input_labels = [\"Title\", \"Query\", \"Source\"]\n",
    "outcome_label = \"relevant_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_labels].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_titles = [sample[0] for sample in X_train]\n",
    "X_train_queries = [sample[1] for sample in X_train]\n",
    "X_train_sources = [sample[2] for sample in X_train]\n",
    "\n",
    "X_test_titles = [sample[0] for sample in X_test]\n",
    "X_test_queries = [sample[1] for sample in X_test]\n",
    "X_test_sources = [sample[2] for sample in X_test]\n",
    "\n",
    "# generate sentence embeddings\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode train \n",
    "X_train_titles_vectorized = encoder.encode(X_train_titles)\n",
    "X_train_queries_vectorized = encoder.encode(X_train_queries)\n",
    "X_train_sources_vectorized = encoder.encode(X_train_sources)\n",
    "X_train_vectorized = np.concatenate((X_train_titles_vectorized, X_train_queries_vectorized, X_train_sources_vectorized), axis=1)\n",
    "\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_titles_vectorized = encoder.encode(X_test_titles)\n",
    "X_test_queries_vectorized = encoder.encode(X_test_queries)\n",
    "X_test_sources_vectorized = encoder.encode(X_test_sources)\n",
    "X_test_vectorized = np.concatenate((X_test_titles_vectorized, X_test_queries_vectorized, X_test_sources_vectorized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_we_multi = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_we_multi.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_we_multi.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_we_multi.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_we_multi.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_we_multi.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (cross validation test - whole dataset)\n",
    "\n",
    "# Encode data\n",
    "X_titles = [sample[0] for sample in X]\n",
    "X_queries = [sample[1] for sample in X]\n",
    "X_sources = [sample[2] for sample in X]\n",
    "\n",
    "X_titles_vectorized = encoder.encode(X_titles)\n",
    "X_queries_vectorized = encoder.encode(X_queries)\n",
    "X_sources_vectorized = encoder.encode(X_sources)\n",
    "X_vectorized = np.concatenate((X_titles_vectorized, X_queries_vectorized, X_sources_vectorized), axis=1)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_cv = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "#good article on why don't select final instance of chosen model using CV, positioned as a evaluation tool only - https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n",
    "    #also suggests that for final model could train it on the whole dataset (thereby maximising the data use)\n",
    "outputs = cross_validate(lr_model_cv, X_vectorized, y, cv=7, scoring=scoring, return_train_score=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_all = LogisticRegression().fit(X_vectorized, y)\n",
    "quick_accuracy_check = lr_model_all.score(X_vectorized, y)\n",
    "print(quick_accuracy_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries - as dummy vars (setup dummy vars)\n",
    "\n",
    "def create_query_dummy_vars(query, query_col_name):\n",
    "    if query == query_col_name:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dummy_query_vals = [\"large language model evaluation\", \"menopause symptoms\", \"ChatGPT for healthcare\", \"menopause prediction\", \"menopause genetics\"]\n",
    "    \n",
    "for query in dummy_query_vals:\n",
    "    agent_list[query] = agent_list[\"Query\"].apply(create_query_dummy_vars, args=(query, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries - as dummy vars (create vectors)\n",
    "\n",
    "input_labels = [\"Title\"] + dummy_query_vals\n",
    "outcome_label = \"relevant_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_labels].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_titles = [sample[0] for sample in X_train]\n",
    "X_train_queries = [list(sample[1:]) for sample in X_train]\n",
    "\n",
    "X_test_titles = [sample[0] for sample in X_test]\n",
    "X_test_queries = [list(sample[1:]) for sample in X_test]\n",
    "\n",
    "# generate sentence embeddings\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode train \n",
    "X_train_titles_vectorized = encoder.encode(X_train_titles)\n",
    "# X_train_queries_vectorized = encoder.encode(X_train_queries)\n",
    "X_train_vectorized = np.concatenate((X_train_titles_vectorized, X_train_queries), axis=1)\n",
    "\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_titles_vectorized = encoder.encode(X_test_titles)\n",
    "# X_test_queries_vectorized = encoder.encode(X_test_queries)\n",
    "X_test_vectorized = np.concatenate((X_test_titles_vectorized, X_test_queries), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_we_multi = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_we_multi.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_we_multi.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_we_multi.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_we_multi.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_we_multi.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT / transformer based fine-tuning (in google colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some reference resources (also explored this in mimic-diag-prediction)\n",
    "    #Huggingface\n",
    "        #Intro - https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "        #Fine-tuning example (but not custom dataset or easily digestable) - https://huggingface.co/docs/transformers/training + https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/training.ipynb\n",
    "        #Text classification example (again not a custom dataset, but could be useful as reference) - https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb\n",
    "    #Other new examples (generally too high level or not fully complete e.g. loading a custom dataset)\n",
    "        #https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python?utm_content=cmp-true\n",
    "        #https://lajavaness.medium.com/regression-with-text-input-using-bert-and-transformers-71c155034b13\n",
    "        #https://saturncloud.io/blog/bert-text-classification-using-pytorch-a-guide-for-data-scientists/\n",
    "    #Legacy examples (from notes, none of these worked comprehensively)\n",
    "        #https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S\n",
    "        #https://colab.research.google.com/github/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb\n",
    "        #reminder that this might not be feasible on an CPU - may need to switch to Google Colab (when run tests it is much, much quicker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT output for below prompt (v2)\n",
    "\n",
    "Provide a complete example in Python of fine-tuning a BERT model on a custom dataset to predict a binary outcome. For example, given a text title, predict whether the title is relevant (Yes or No). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### full based off original (code in google colab runs quickly on full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your custom dataset in two lists: titles and labels (0 or 1)\n",
    "#NOTE: shortened until proved works as expect\n",
    "input_label = \"Title\"\n",
    "outcome_label = \"relevant_bin\"\n",
    "titles = agent_list[input_label].values[0:100]\n",
    "labels = agent_list[outcome_label].values[0:100]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_titles, test_titles, train_labels, test_labels = train_test_split(titles, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom Dataset class for loading the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, titles, labels, tokenizer, max_length):\n",
    "        self.titles = titles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Set the maximum sequence length for BERT input\n",
    "max_length = 128\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the custom train and test datasets\n",
    "train_dataset = CustomDataset(train_titles, train_labels, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(test_titles, test_labels, tokenizer, max_length)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up the optimizer and the device (assuming GPU is available)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch #: \", epoch)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        print(\"Train batch #: \", idx)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            print(\"Test batch #: \", idx)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            print(\"Test outputs: \", outputs)\n",
    "            logits = outputs.logits\n",
    "            preds = F.softmax(logits, dim=1).argmax(dim=1)\n",
    "            print(\"Test labels: \", labels)\n",
    "            print(\"Test preds: \", preds)\n",
    "\n",
    "            num_correct += torch.sum(preds == labels).item()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = num_correct / len(test_dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Avg. training loss: {avg_train_loss:.4f}, Test accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Calculate AUC and confusion matrix\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f'AUC: {auc_score:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Calculate sensitivity and specificity\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(f'Sensitivity (True Positive Rate): {sensitivity:.4f}')\n",
    "    print(f'Specificity (True Negative Rate): {specificity:.4f}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
