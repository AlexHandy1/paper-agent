{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary notes\n",
    "- Strong logistic regression benchmarks for \"Relevant\" predictions (>80% accuracy offline). Performs better than Xgboost and BERT [in google colab] (without extensive hyperparameter tuning).\n",
    "- Could improve further by tidying up input labeling (e.g. remove all vision / general neural network stuff)\n",
    "- Should also implement full cross validation to ensure consistent performance across sample (random tests suggests does meaningfully differ).\n",
    "- Adding query and source to title feature both as embeddings and dummy vars only very marginally improves performance (so leave out in first prod version).\n",
    "- In production, currently using a logistic regression model to classify whether an article is relevant (Y or N) that takes word embeddings of the article title as the input feature. \n",
    "- Predicting \"Read\" harder as <5% of cases and probably inconsistent rationale (basic log reg model just predicts all not read), so more of an outlier detection piece (may need different approach). \n",
    "- Difficult deployment / file size constraints using basic (free) AWS Lambda file upload where currently have to upload all python depedencies (huge files for sentence transforers / embeddings). Replaced with an AWS ECR deployment approach (details in README.md) which incurs a small hosting cost (~1USD / month).\n",
    "- Performance in production of v1 model to 6th Dec 2023, better at identifying truly relevant articles (true positives in sensitivity) vs truly non relevant articles (true negatives in specificity). Preferred weighting as would rather have false positives than false negatives. \n",
    "    Accuracy:  0.8700389105058366\n",
    "    Sensitivity: 0.9118303571428571\n",
    "    Specificity: 0.8476702508960573\n",
    "\n",
    "#### Next steps\n",
    "- Review model performance again on new queries\n",
    "- Add back in query as a feature (vectors and dummy vars) and conduct a more granular performance review by query\n",
    "- Extend multi-feature approach to BERT + explore more tuning to improve performance\n",
    "- Research data outlier techniques to improve \"Read\" prediction or class weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold, cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions (could move)\n",
    "\n",
    "def convert_outcome_to_bin(x):\n",
    "    if x == \"Y\":\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def evaluate(target, prediction, prediction_prob):\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "\n",
    "    # Calculate the AUC score\n",
    "    auc_score = roc_auc_score(target, prediction_prob)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(target, prediction)\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    true_negative = conf_matrix[0, 0]\n",
    "    false_positive = conf_matrix[0, 1]\n",
    "    false_negative = conf_matrix[1, 0]\n",
    "    true_positive = conf_matrix[1, 1]\n",
    "\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"AUC: {auc_score:.2f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "#NEED TO ABSTRACT TO TARGET OUTCOME\n",
    "def flag_pred_error(x):\n",
    "    target = x[\"relevant_true\"]\n",
    "    pred = x[\"relevant_pred\"]\n",
    "    \n",
    "    if pred == target:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "# input_agent_file = \"paper_agent_list_260723.csv\"\n",
    "input_agent_file = \"paper_agent_list_061223.csv\"\n",
    "agent_list = pd.read_csv(input_agent_file)\n",
    "agent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy abstract so could use as an input feature\n",
    "agent_list[\"Abstract_clean\"] = agent_list[\"Abstract\"].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert target outcomes to binary features\n",
    "\n",
    "agent_list[\"relevant_bin\"] = agent_list[\"Relevant?\"].apply(convert_outcome_to_bin)\n",
    "print(agent_list[\"relevant_bin\"].value_counts())\n",
    "\n",
    "agent_list[\"read_bin\"] = agent_list[\"Read?\"].apply(convert_outcome_to_bin)\n",
    "agent_list[\"read_bin\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing production models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## raw performance metrics\n",
    "\n",
    "#filter to only data where predictons were made\n",
    "agent_list_relevant_pred_df = agent_list[pd.notna(agent_list[\"Relevant_pred\"])]\n",
    "\n",
    "#quick accuracy calc\n",
    "total_preds = len(agent_list_relevant_pred_df)\n",
    "agrees = np.sum(agent_list_relevant_pred_df[\"Relevant_pred\"] == agent_list_relevant_pred_df[\"Relevant?\"])\n",
    "print(\"Accuracy: \", agrees / total_preds)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(agent_list_relevant_pred_df['Relevant?'], agent_list_relevant_pred_df['Relevant_pred'], labels=['Y', 'N'])\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "true_positives = conf_matrix[0, 0]\n",
    "false_negatives = conf_matrix[0, 1]\n",
    "true_negatives = conf_matrix[1, 1]\n",
    "false_positives = conf_matrix[1, 0]\n",
    "\n",
    "# Calculate sensitivity (true positive rate)\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Calculate specificity (true negative rate)\n",
    "specificity = true_negatives / (true_negatives + false_positives)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reviewing disagreements\n",
    "\n",
    "pred_disagreements = agent_list_relevant_pred_df[agent_list_relevant_pred_df[\"Relevant_pred\"] != agent_list_relevant_pred_df[\"Relevant?\"]]\n",
    "\n",
    "\n",
    "#visual evaluation if wanted to explore more specific performance details. \n",
    "    #could also calculate metrics by different queries and sources\n",
    "# for idx, row in pred_disagreements.iterrows():\n",
    "#     print(row[\"Title\"] + \" > \" + row[\"Query\"])\n",
    "#     print(\"\\n\")\n",
    "# pred_disagreements[[\"Title\", \"Source\", \"Query\", \"Relevant_pred\", \"Relevant?\", \"Read?\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training offline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count matrix as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data\n",
    "\n",
    "input_label = \"Title\"\n",
    "# input_label = \"Abstract_clean\" \n",
    "    #marginally more accurate (much better sensitivity e.g. misses less relevant articles), in optimum model should aim to include title, abstract and original query. \n",
    "    #Interestingly exactly the same number of errors (43 - suggests labeling inconsistency?)\n",
    "outcome_label = \"relevant_bin\"\n",
    "# outcome_label = \"read_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_label].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CountVectorizer to convert text data into numerical features\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model\n",
    "lr_model_cm = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_cm.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_cm.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_cm.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_cm.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_cm.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surface incorrect examples\n",
    "#NOTE: some of this suggests labeling error\n",
    "\n",
    "test_examples_dict = {\"titles\": X_test, \"relevant_true\": y_test, \"relevant_pred\":y_pred}\n",
    "test_examples_df = pd.DataFrame(test_examples_dict)\n",
    "\n",
    "test_examples_df[\"pred_error\"] = test_examples_df.apply(flag_pred_error, axis=1)\n",
    "test_examples_df_errors = test_examples_df[test_examples_df[\"pred_error\"] == 1]\n",
    "test_examples_df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: actually don't disagree with many of these, further supports that for best performance will likely need to refine input labels\n",
    "test_examples_df_errors[test_examples_df_errors[\"relevant_true\"] == 0][\"titles\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (cross validation test - whole dataset)\n",
    "\n",
    "# Encode data\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_cv = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "#good article on why don't select final instance of chosen model using CV, positioned as a evaluation tool only - https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n",
    "    #also suggests that for final model could train it on the whole dataset (thereby maximising the data use)\n",
    "outputs = cross_validate(lr_model_cv, X_vectorized, y, cv=7, scoring=scoring, return_train_score=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model on all data\n",
    "\n",
    "lr_model_wc = LogisticRegression().fit(X_vectorized, y)\n",
    "\n",
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "lr_model_wc_filename = 'pa_lr_model_wc_all_data_' + today_date + \".pkl\"\n",
    "pickle.dump(lr_model_wc, open(lr_model_wc_filename, 'wb'))\n",
    "\n",
    "#test load (comment out)\n",
    "lr_model_wc_loaded = pickle.load(open(lr_model_wc_filename, 'rb'))\n",
    "loaded_result = lr_model_wc_loaded.score(X_vectorized, y)\n",
    "print(loaded_result)\n",
    "\n",
    "lr_model_wc_loaded.predict(X_vectorized[2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings as feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data\n",
    "\n",
    "input_label = \"Title\"\n",
    "# input_label = \"Abstract_clean\" \n",
    "    #Takes significantly longer to embed - something to factor into application (would need to add an order of magnitude more accuracy to be worth computing on the fly)\n",
    "    #Actually performs worse on this subset\n",
    "outcome_label = \"relevant_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_label].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#generate sentence embeddings\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode train \n",
    "X_train_vectorized = encoder.encode(X_train)\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_vectorized = encoder.encode(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (basic train test split to review cases / errors)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_we = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_we.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_we.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_we.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_we.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_we.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "lr_model_we_filename = 'pa_lr_model_we_tt_data_' + today_date + \".pkl\"\n",
    "pickle.dump(lr_model_we, open(lr_model_we_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load (comment out)\n",
    "lr_model_we_loaded = pickle.load(open(lr_model_we_filename, 'rb'))\n",
    "loaded_result = lr_model_we_loaded.score(X_test_vectorized, y_test)\n",
    "print(loaded_result)\n",
    "\n",
    "lr_model_we_loaded.predict(X_test_vectorized[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect errors\n",
    "\n",
    "test_examples_dict = {\"titles\": X_test, \"relevant_true\": y_test, \"relevant_pred\":y_pred}\n",
    "test_examples_df = pd.DataFrame(test_examples_dict)\n",
    "\n",
    "test_examples_df[\"pred_error\"] = test_examples_df.apply(flag_pred_error, axis=1)\n",
    "test_examples_df_errors = test_examples_df[test_examples_df[\"pred_error\"] == 1]\n",
    "test_examples_df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (cross validation test - whole dataset)\n",
    "\n",
    "# Encode data\n",
    "X_vectorized = encoder.encode(X)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_cv = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "#good article on why don't select final instance of chosen model using CV, positioned as a evaluation tool only - https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n",
    "    #also suggests that for final model could train it on the whole dataset (thereby maximising the data use)\n",
    "outputs = cross_validate(lr_model_cv, X_vectorized, y, cv=7, scoring=scoring, return_train_score=True, return_estimator=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model fit on entire dataset (refer to CV for estimated performance)\n",
    "\n",
    "lr_model_we_all = LogisticRegression()\n",
    "lr_model_we_all.fit(X_vectorized, y)\n",
    "\n",
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "lr_model_we_all_filename = 'pa_lr_model_we_' + today_date + \".pkl\"\n",
    "pickle.dump(lr_model_we_all, open(lr_model_we_all_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load (comment out)\n",
    "lr_model_we_all_loaded = pickle.load(open(lr_model_we_all_filename, 'rb'))\n",
    "loaded_result = lr_model_we_all_loaded.score(X_vectorized, y)\n",
    "print(loaded_result)\n",
    "\n",
    "lr_model_we_all_loaded.predict(X_vectorized[2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_we = xgb.XGBRegressor(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "xgb_model_we.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train_prob = xgb_model_we.predict(X_train_vectorized)\n",
    "y_pred_train = [ 1 if p >= 0.5 else 0 for p in y_pred_train_prob ]\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = xgb_model_we.predict(X_test_vectorized)\n",
    "y_pred = [ 1 if p >= 0.5 else 0 for p in y_pred_prob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "today_date = datetime.today().strftime('%Y_%m_%d')\n",
    "xgb_model_we_filename = 'pa_xgb_model_we_' + today_date + \".pkl\"\n",
    "pickle.dump(xgb_model_we, open(xgb_model_we_filename, 'wb'))\n",
    "\n",
    "#test load (comment out)\n",
    "\n",
    "# xgb_model_we_loaded = pickle.load(open(xgb_model_we_filename, 'rb'))\n",
    "# loaded_result = xgb_model_we_loaded.score(X_test_vectorized, y_test)\n",
    "# print(loaded_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect errors\n",
    "\n",
    "test_examples_dict = {\"titles\": X_test, \"relevant_true\": y_test, \"relevant_pred\":y_pred}\n",
    "test_examples_df = pd.DataFrame(test_examples_dict)\n",
    "\n",
    "test_examples_df[\"pred_error\"] = test_examples_df.apply(flag_pred_error, axis=1)\n",
    "test_examples_df_errors = test_examples_df[test_examples_df[\"pred_error\"] == 1]\n",
    "test_examples_df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries and sources - as word embeddings\n",
    "\n",
    "input_labels = [\"Title\", \"Query\", \"Source\"]\n",
    "outcome_label = \"relevant_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_labels].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_titles = [sample[0] for sample in X_train]\n",
    "X_train_queries = [sample[1] for sample in X_train]\n",
    "X_train_sources = [sample[2] for sample in X_train]\n",
    "\n",
    "X_test_titles = [sample[0] for sample in X_test]\n",
    "X_test_queries = [sample[1] for sample in X_test]\n",
    "X_test_sources = [sample[2] for sample in X_test]\n",
    "\n",
    "# generate sentence embeddings\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode train \n",
    "X_train_titles_vectorized = encoder.encode(X_train_titles)\n",
    "X_train_queries_vectorized = encoder.encode(X_train_queries)\n",
    "X_train_sources_vectorized = encoder.encode(X_train_sources)\n",
    "X_train_vectorized = np.concatenate((X_train_titles_vectorized, X_train_queries_vectorized, X_train_sources_vectorized), axis=1)\n",
    "\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_titles_vectorized = encoder.encode(X_test_titles)\n",
    "X_test_queries_vectorized = encoder.encode(X_test_queries)\n",
    "X_test_sources_vectorized = encoder.encode(X_test_sources)\n",
    "X_test_vectorized = np.concatenate((X_test_titles_vectorized, X_test_queries_vectorized, X_test_sources_vectorized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_we_multi = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_we_multi.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_we_multi.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_we_multi.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_we_multi.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_we_multi.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model (cross validation test - whole dataset)\n",
    "\n",
    "# Encode data\n",
    "X_titles = [sample[0] for sample in X]\n",
    "X_queries = [sample[1] for sample in X]\n",
    "X_sources = [sample[2] for sample in X]\n",
    "\n",
    "X_titles_vectorized = encoder.encode(X_titles)\n",
    "X_queries_vectorized = encoder.encode(X_queries)\n",
    "X_sources_vectorized = encoder.encode(X_sources)\n",
    "X_vectorized = np.concatenate((X_titles_vectorized, X_queries_vectorized, X_sources_vectorized), axis=1)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_cv = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc', 'precision', 'recall']\n",
    "#good article on why don't select final instance of chosen model using CV, positioned as a evaluation tool only - https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n",
    "    #also suggests that for final model could train it on the whole dataset (thereby maximising the data use)\n",
    "outputs = cross_validate(lr_model_cv, X_vectorized, y, cv=7, scoring=scoring, return_train_score=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_all = LogisticRegression().fit(X_vectorized, y)\n",
    "quick_accuracy_check = lr_model_all.score(X_vectorized, y)\n",
    "print(quick_accuracy_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries - as dummy vars (setup dummy vars)\n",
    "\n",
    "def create_query_dummy_vars(query, query_col_name):\n",
    "    if query == query_col_name:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dummy_query_vals = [\"large language model evaluation\", \"menopause symptoms\", \"ChatGPT for healthcare\", \"menopause prediction\", \"menopause genetics\"]\n",
    "    \n",
    "for query in dummy_query_vals:\n",
    "    agent_list[query] = agent_list[\"Query\"].apply(create_query_dummy_vars, args=(query, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries - as dummy vars (create vectors)\n",
    "\n",
    "input_labels = [\"Title\"] + dummy_query_vals\n",
    "outcome_label = \"relevant_bin\"\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = agent_list[input_labels].values\n",
    "y = agent_list[outcome_label].values\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_titles = [sample[0] for sample in X_train]\n",
    "X_train_queries = [list(sample[1:]) for sample in X_train]\n",
    "\n",
    "X_test_titles = [sample[0] for sample in X_test]\n",
    "X_test_queries = [list(sample[1:]) for sample in X_test]\n",
    "\n",
    "# generate sentence embeddings\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode train \n",
    "X_train_titles_vectorized = encoder.encode(X_train_titles)\n",
    "# X_train_queries_vectorized = encoder.encode(X_train_queries)\n",
    "X_train_vectorized = np.concatenate((X_train_titles_vectorized, X_train_queries), axis=1)\n",
    "\n",
    "\n",
    "# Transform the testing data using the same vectorizer\n",
    "X_test_titles_vectorized = encoder.encode(X_test_titles)\n",
    "# X_test_queries_vectorized = encoder.encode(X_test_queries)\n",
    "X_test_vectorized = np.concatenate((X_test_titles_vectorized, X_test_queries), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model_we_multi = LogisticRegression()\n",
    "\n",
    "# Train the model using the vectorized training data\n",
    "lr_model_we_multi.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_pred_train = lr_model_we_multi.predict(X_train_vectorized)\n",
    "y_pred_train_prob = lr_model_we_multi.predict_proba(X_train_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model_we_multi.predict(X_test_vectorized)\n",
    "y_pred_prob = lr_model_we_multi.predict_proba(X_test_vectorized)[:, 1]  # Probability of class 1 (Relevant)\n",
    "\n",
    "#evaluate\n",
    "evaluate(y_test, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT / transformer based fine-tuning (in google colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some reference resources (also explored this in mimic-diag-prediction)\n",
    "    #Huggingface\n",
    "        #Intro - https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "        #Fine-tuning example (but not custom dataset or easily digestable) - https://huggingface.co/docs/transformers/training + https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/training.ipynb\n",
    "        #Text classification example (again not a custom dataset, but could be useful as reference) - https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb\n",
    "    #Other new examples (generally too high level or not fully complete e.g. loading a custom dataset)\n",
    "        #https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python?utm_content=cmp-true\n",
    "        #https://lajavaness.medium.com/regression-with-text-input-using-bert-and-transformers-71c155034b13\n",
    "        #https://saturncloud.io/blog/bert-text-classification-using-pytorch-a-guide-for-data-scientists/\n",
    "    #Legacy examples (from notes, none of these worked comprehensively)\n",
    "        #https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S\n",
    "        #https://colab.research.google.com/github/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb\n",
    "        #reminder that this might not be feasible on an CPU - may need to switch to Google Colab (when run tests it is much, much quicker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT output for below prompt (v2)\n",
    "\n",
    "Provide a complete example in Python of fine-tuning a BERT model on a custom dataset to predict a binary outcome. For example, given a text title, predict whether the title is relevant (Yes or No). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### full based off original (code in google colab runs quickly on full dataset)\n",
    "https://colab.research.google.com/drive/1feZ5mjrQyHe5tOyTWbmzNxqgFHENjdwV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your custom dataset in two lists: titles and labels (0 or 1)\n",
    "#NOTE: shortened until proved works as expect\n",
    "input_label = \"Title\"\n",
    "outcome_label = \"relevant_bin\"\n",
    "titles = agent_list[input_label].values[0:100]\n",
    "labels = agent_list[outcome_label].values[0:100]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_titles, test_titles, train_labels, test_labels = train_test_split(titles, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom Dataset class for loading the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, titles, labels, tokenizer, max_length):\n",
    "        self.titles = titles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Set the maximum sequence length for BERT input\n",
    "max_length = 128\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the custom train and test datasets\n",
    "train_dataset = CustomDataset(train_titles, train_labels, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(test_titles, test_labels, tokenizer, max_length)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up the optimizer and the device (assuming GPU is available)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch #: \", epoch)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        print(\"Train batch #: \", idx)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            print(\"Test batch #: \", idx)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            print(\"Test outputs: \", outputs)\n",
    "            logits = outputs.logits\n",
    "            preds = F.softmax(logits, dim=1).argmax(dim=1)\n",
    "            print(\"Test labels: \", labels)\n",
    "            print(\"Test preds: \", preds)\n",
    "\n",
    "            num_correct += torch.sum(preds == labels).item()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = num_correct / len(test_dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Avg. training loss: {avg_train_loss:.4f}, Test accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Calculate AUC and confusion matrix\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f'AUC: {auc_score:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Calculate sensitivity and specificity\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(f'Sensitivity (True Positive Rate): {sensitivity:.4f}')\n",
    "    print(f'Specificity (True Negative Rate): {specificity:.4f}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
